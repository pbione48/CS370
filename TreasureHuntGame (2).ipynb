{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treasure Hunt Game Notebook\n",
    "\n",
    "## Read and Review Your Starter Code\n",
    "The theme of this project is a popular treasure hunt game in which the player needs to find the treasure before the pirate does. While you will not be developing the entire game, you will write the part of the game that represents the intelligent agent, which is a pirate in this case. The pirate will try to find the optimal path to the treasure using deep Q-learning. \n",
    "\n",
    "You have been provided with two Python classes and this notebook to help you with this assignment. The first class, TreasureMaze.py, represents the environment, which includes a maze object defined as a matrix. The second class, GameExperience.py, stores the episodes – that is, all the states that come in between the initial state and the terminal state. This is later used by the agent for learning by experience, called \"exploration\". This notebook shows how to play a game. Your task is to complete the deep Q-learning implementation for which a skeleton implementation has been provided. The code blocks you will need to complete has #TODO as a header.\n",
    "\n",
    "First, read and review the next few code and instruction blocks to understand the code that you have been given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Imports\n",
    "This section imports all required Python libraries and project modules.  \n",
    "- **NumPy** for numerical operations  \n",
    "- **Matplotlib** for plotting and visualization  \n",
    "- **TensorFlow/Keras** for building and training the neural network  \n",
    "- **TreasureMaze** and **GameExperience** are provided project files to define the environment and replay buffer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 3 already has print() — this future import isn’t needed, but harmless\n",
    "# from __future__ import print_function\n",
    "\n",
    "import os, sys, time, datetime, json, random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use tensorflow.keras consistently\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Activation, PReLU\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "\n",
    "# If you use the functional API elsewhere, you can also import:\n",
    "# from tensorflow.keras import layers, models, optimizers\n",
    "\n",
    "from TreasureMaze import TreasureMaze\n",
    "from GameExperience import GameExperience\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Resume Function\n",
    "This helper function reloads the latest checkpointed model if one exists, or builds a fresh model otherwise.  \n",
    "It allows training to continue across multiple sessions without losing progress.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "def resume_model():\n",
    "    \"\"\"\n",
    "    Load the latest checkpoint if it exists, else build a fresh model.\n",
    "    Returns (model, start_epoch).\n",
    "    \"\"\"\n",
    "    ckpt, start_epoch = latest_checkpoint()\n",
    "    if ckpt:\n",
    "        model = load_model(ckpt)\n",
    "        print(f\"Resuming from {ckpt} (epoch {start_epoch})\")\n",
    "    else:\n",
    "        model = build_model(qmaze)\n",
    "        start_epoch = 0\n",
    "        print(\"Starting fresh with new model\")\n",
    "    return model, start_epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directory Setup and Logging\n",
    "Creates folders for saving model checkpoints and logs.  \n",
    "Also defines a simple logging function (`log`) to keep training output consistent and easy to read.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folders (ok if they already exist)\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "def log(msg):\n",
    "    print(msg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maze Definition\n",
    "Defines the maze as a NumPy array, initializes the `TreasureMaze` environment,  \n",
    "and prints the treasure target location along with an example valid starting cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = np.array([\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  0.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  0.,  1.,  0.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  0.,  0.,  0.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  0.,  1.,  0.,  1.]\n",
    "])\n",
    "\n",
    "qmaze = TreasureMaze(maze)\n",
    "\n",
    "print(\"Target:\", qmaze.target)\n",
    "print(\"Random valid start example:\", random.choice(qmaze.free_cells))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maze Visualization\n",
    "Provides a function to display the maze grid using matplotlib.  \n",
    "Visited cells are shaded, the pirate’s current location is marked, and the treasure is highlighted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(qmaze):\n",
    "    plt.grid('on')\n",
    "    nrows, ncols = qmaze.maze.shape\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(0.5, nrows, 1))\n",
    "    ax.set_yticks(np.arange(0.5, ncols, 1))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    canvas = np.copy(qmaze.maze)\n",
    "    for row, col in qmaze.visited:\n",
    "        canvas[row, col] = 0.6\n",
    "    pirate_row, pirate_col, _ = qmaze.state\n",
    "    canvas[pirate_row, pirate_col] = 0.3     # pirate\n",
    "    canvas[nrows-1, ncols-1] = 0.9           # treasure\n",
    "    img = plt.imshow(canvas, interpolation='none', cmap='gray')\n",
    "    return img\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions and Constants\n",
    "Defines the four movement actions (`LEFT, UP, RIGHT, DOWN`) and assigns them integer values.  \n",
    "Also creates a dictionary for human-readable labels and sets the initial exploration factor (`epsilon`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This helper function allows a visual representation of the maze object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEFT, UP, RIGHT, DOWN = 0, 1, 2, 3\n",
    "actions_dict = {LEFT:'left', UP:'up', RIGHT:'right', DOWN:'down'}\n",
    "epsilon = 0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Function\n",
    "Ensures that input state data is reshaped into the correct batch format for the neural network’s `predict` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def as_batch(x):\n",
    "    import numpy as np\n",
    "    x = np.asarray(x)\n",
    "    return x if x.ndim == 2 else x.reshape(1, -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Model\n",
    "Defines a Deep Q-Network (DQN) with two hidden layers (64 neurons each).  \n",
    "The model predicts Q-values for each possible action given the maze state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(qmaze):\n",
    "    state_dim = qmaze.maze.size\n",
    "    n_actions = 4\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=state_dim, activation=\"relu\"))\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "    model.add(Dense(n_actions, activation=\"linear\"))\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-3), loss=\"mse\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gameplay Functions\n",
    "- **play_game**: Runs a full game episode starting from a given cell using the current model. Returns the outcome and path.  \n",
    "- **completion_check**: Verifies if the trained agent can solve the maze consistently from all valid starting positions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(model, qmaze, pirate_cell):\n",
    "    qmaze.reset(pirate_cell)\n",
    "    envstate = qmaze.observe()\n",
    "    path = [pirate_cell]\n",
    "    while True:\n",
    "        q = model.predict(as_batch(envstate), verbose=0)[0]\n",
    "\n",
    "        action = int(np.argmax(q))\n",
    "        envstate, reward, game_status = qmaze.act(action)\n",
    "        pr, pc, _ = qmaze.state\n",
    "        path.append((pr, pc))\n",
    "        if game_status == 'win':\n",
    "            return True, path\n",
    "        elif game_status == 'lose':\n",
    "            return False, path\n",
    "\n",
    "def completion_check(model, qmaze):\n",
    "    for cell in qmaze.free_cells:\n",
    "        if not qmaze.valid_actions(cell):\n",
    "            return False\n",
    "        won, _ = play_game(model, qmaze, cell)\n",
    "        if not won:\n",
    "            return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Function\n",
    "Runs multiple episodes from random starting positions and calculates the average win rate of the agent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, qmaze, n_episodes=100):\n",
    "    wins = 0\n",
    "    for _ in range(n_episodes):\n",
    "        start = random.choice(qmaze.free_cells)\n",
    "        won, _ = play_game(model, qmaze, pirate_cell=start)\n",
    "        if won:\n",
    "            wins += 1\n",
    "    return wins / float(n_episodes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Function (Deep Q-Learning)\n",
    "Implements the reinforcement learning loop using:  \n",
    "- Experience Replay (via `GameExperience`)  \n",
    "- ε-greedy policy with decay for exploration vs. exploitation  \n",
    "- Neural network Q-value updates using the Bellman equation  \n",
    "- Logging and checkpoint saving for progress tracking  \n",
    "- Optional early stopping if the agent reaches consistent perfect performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qtrain(model, qmaze, *,\n",
    "           n_epoch_total=10000,\n",
    "           start_epoch=0,\n",
    "           max_memory=4000,\n",
    "           batch_size=16,\n",
    "           target_update_steps=500,    # kept for compatibility (not used with GameExperience)\n",
    "           checkpoint_every=500,\n",
    "           max_steps_per_episode=200,\n",
    "           log_every=200):\n",
    "    \"\"\"\n",
    "    Training loop using the provided GameExperience replay buffer.\n",
    "    Expects:\n",
    "      - GameExperience(model, max_memory=...) with .remember([s,a,r,s',done]) and .get_data(batch_size)\n",
    "      - qmaze.reset(start_cell), qmaze.observe(), qmaze.act(action) -> (next_state, reward, status)\n",
    "      - status is 'win' or 'lose' when terminal\n",
    "      - as_batch(x) utility available to ensure (batch, features) input to model.predict(...)\n",
    "      - log(msg) available for progress messages\n",
    "    \"\"\"\n",
    "\n",
    "    import datetime\n",
    "    import numpy as np\n",
    "    import random\n",
    "\n",
    "    # Replay buffer using your existing class\n",
    "    experience = GameExperience(model, max_memory=max_memory)\n",
    "\n",
    "    # Simple epsilon (exploration) schedule\n",
    "    eps_start = 1.0 if start_epoch == 0 else 0.2\n",
    "    eps_min   = 0.05\n",
    "    eps_decay = 0.997\n",
    "    epsilon   = max(eps_min, eps_start * (eps_decay ** start_epoch))\n",
    "\n",
    "    # Rolling window to estimate recent win-rate\n",
    "    win_history = []\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    def fmt_time(seconds):\n",
    "        s = int(seconds)\n",
    "        if s < 60: return f\"{s}s\"\n",
    "        m, s = divmod(s, 60)\n",
    "        if m < 60: return f\"{m}m{s:02d}s\"\n",
    "        h, m = divmod(m, 60); return f\"{h}h{m:02d}m{s:02d}s\"\n",
    "\n",
    "    for ep in range(start_epoch, n_epoch_total):\n",
    "        # random valid start each episode\n",
    "        start_cell = random.choice(qmaze.free_cells)\n",
    "        qmaze.reset(start_cell)\n",
    "        envstate = qmaze.observe()\n",
    "\n",
    "        steps = 0\n",
    "        game_over = False\n",
    "        loss = 0.0\n",
    "        last_reward = 0.0\n",
    "        status = None\n",
    "\n",
    "        while not game_over and steps < max_steps_per_episode:\n",
    "            steps += 1\n",
    "\n",
    "            # ε-greedy policy\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = random.choice([0, 1, 2, 3])  # LEFT, UP, RIGHT, DOWN\n",
    "            else:\n",
    "                q_vals = model.predict(as_batch(envstate), verbose=0)[0]\n",
    "                action = int(np.argmax(q_vals))\n",
    "\n",
    "            prev_envstate = envstate\n",
    "            envstate, reward, status = qmaze.act(action)\n",
    "            done = (status in ('win', 'lose'))\n",
    "            last_reward = reward\n",
    "\n",
    "            # store transition and train on a mini-batch\n",
    "            experience.remember([prev_envstate, action, reward, envstate, done])\n",
    "            X, y = experience.get_data(batch_size)\n",
    "            if X is not None and y is not None and len(X) > 0:\n",
    "                loss = float(model.train_on_batch(X, y))\n",
    "\n",
    "            game_over = done\n",
    "\n",
    "        # update epsilon (decay) at end of episode\n",
    "        epsilon = max(eps_min, epsilon * eps_decay)\n",
    "\n",
    "        # record win/lose\n",
    "        win_history.append(1 if status == 'win' else 0)\n",
    "        wr_window = win_history[-min(200, len(win_history)):]  # last up to 200 episodes\n",
    "        wr200 = np.mean(wr_window) if wr_window else 0.0\n",
    "\n",
    "        # log progress\n",
    "        if (ep - start_epoch) % log_every == 0:\n",
    "            dt = (datetime.datetime.now() - start_time).total_seconds()\n",
    "            log(f\"epoch {ep:05d} | eps={epsilon:.3f} | loss={loss:.4f} | steps={steps} | \"\n",
    "                f\"reward={last_reward:.2f} | wr200={wr200:.2%} | time={fmt_time(dt)}\")\n",
    "\n",
    "        # checkpoint\n",
    "        if (ep - start_epoch) % checkpoint_every == 0 and ep > start_epoch:\n",
    "            save_path = f\"checkpoints/pirate_epoch{ep}.h5\"\n",
    "            model.save(save_path)\n",
    "            log(f\"[ckpt] saved -> {save_path}\")\n",
    "\n",
    "        # optional: early stop if perfect in the window\n",
    "        if len(wr_window) == 200 and sum(wr_window) == 200:\n",
    "            # (optional) double-check full completion if you want:\n",
    "            # if completion_check(model, qmaze): \n",
    "            save_path = f\"checkpoints/pirate_epoch{ep}.h5\"\n",
    "            model.save(save_path)\n",
    "            log(f\"[early-stop] 100% wr200 at epoch {ep}. Saved -> {save_path}\")\n",
    "            break\n",
    "\n",
    "    # final save at the target epoch\n",
    "    save_path = f\"checkpoints/pirate_epoch{max(ep, start_epoch)}.h5\"\n",
    "    model.save(save_path)\n",
    "    log(f\"[train] finished -> {save_path}\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint Management\n",
    "Utility function to locate the most recent saved model checkpoint.  \n",
    "This enables resuming training from the latest saved epoch instead of starting over.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, re\n",
    "\n",
    "def latest_checkpoint():\n",
    "    ckpts = glob.glob(\"checkpoints/pirate_epoch*.h5\")\n",
    "    if not ckpts:\n",
    "        return None, 0\n",
    "    latest = max(ckpts, key=lambda p: int(re.search(r'epoch(\\d+)', p).group(1)))\n",
    "    start_epoch = int(re.search(r'epoch(\\d+)', latest).group(1))\n",
    "    return latest, start_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_to(total_epochs):\n",
    "    \"\"\"\n",
    "    Reload latest checkpoint (or build fresh), then train until total_epochs.\n",
    "    Use in chunks: 2000 -> 5000 -> 15000, etc.\n",
    "    \"\"\"\n",
    "    ckpt, start_epoch = latest_checkpoint()\n",
    "    if ckpt:\n",
    "        model = load_model(ckpt)\n",
    "        log(f\"Resuming from {ckpt} (epoch {start_epoch})\")\n",
    "    else:\n",
    "        model = build_model(qmaze)\n",
    "        start_epoch = 0\n",
    "        log(\"Starting fresh\")\n",
    "\n",
    "    if start_epoch >= total_epochs:\n",
    "        log(f\"Already at {start_epoch} >= target {total_epochs}; skipping.\")\n",
    "        return model, start_epoch\n",
    "\n",
    "    model = qtrain(\n",
    "        model, qmaze,\n",
    "        n_epoch_total=total_epochs,\n",
    "        start_epoch=start_epoch,\n",
    "        max_memory=2000,\n",
    "        batch_size=16,\n",
    "        target_update_steps=500,\n",
    "        checkpoint_every=200,\n",
    "        max_steps_per_episode=150,\n",
    "        log_every=100\n",
    "    )\n",
    "    return model, total_epochs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training in Chunks (Example Run to 2000 Epochs)\n",
    "Here we demonstrate how to train the agent in smaller steps rather than one very long run.  \n",
    "This helps in case the virtual lab disconnects, since we can resume from checkpoints instead of starting over.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: go to epoch 2000 total\n",
    "model, _ = train_to(2000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint Resume\n",
    "Before continuing training, check if a previous checkpoint exists.  \n",
    "This allows training to pick up from where it left off instead of restarting from epoch 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt, start_epoch = latest_checkpoint()\n",
    "print(\"latest ckpt:\", ckpt, \"| start_epoch:\", start_epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probe Training (10 Epochs)\n",
    "Run a short 10-epoch training \"probe\" to verify that the model is loading correctly  \n",
    "and that training updates are working before committing to a longer run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- quick 10-epoch probe from epoch 500 ----\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# load the checkpointed model (or build fresh if none)\n",
    "model = load_model(ckpt) if ckpt else build_model(qmaze)\n",
    "\n",
    "model = qtrain(\n",
    "    model, qmaze,\n",
    "    n_epoch_total=start_epoch + 10,   # go to 510 total\n",
    "    start_epoch=start_epoch,\n",
    "    max_memory=2000,\n",
    "    batch_size=16,\n",
    "    target_update_steps=500,\n",
    "    checkpoint_every=50,              # save often during probe\n",
    "    max_steps_per_episode=120,        # a bit smaller => faster\n",
    "    log_every=1                       # print every epoch\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continue Training (to 2000 Total Epochs)\n",
    "Extend training from the probe run to reach a total of 2000 epochs.  \n",
    "This balances training time with checkpoint safety and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- continue training to 2000 total epochs ----\n",
    "model = qtrain(\n",
    "    model, qmaze,\n",
    "    n_epoch_total=2000,               # absolute target\n",
    "    start_epoch=start_epoch + 10,     # continue after the probe\n",
    "    max_memory=2000,\n",
    "    batch_size=16,\n",
    "    target_update_steps=500,\n",
    "    checkpoint_every=200,\n",
    "    max_steps_per_episode=150,\n",
    "    log_every=20                      # log every 20 epochs now\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint Progress\n",
    "Check the latest saved checkpoint after training to confirm progress and log the current epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt, start_epoch = latest_checkpoint()\n",
    "print(\"latest ckpt:\", ckpt, \"| start_epoch:\", start_epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Probe Run\n",
    "If needed, reload the latest checkpoint and perform another short 10-epoch probe.  \n",
    "This is useful to verify the model continues learning when resuming at later stages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "ckpt, start_epoch = latest_checkpoint()\n",
    "model = load_model(ckpt) if ckpt else build_model(qmaze)\n",
    "\n",
    "model = qtrain(\n",
    "    model, qmaze,\n",
    "    n_epoch_total=start_epoch + 10,   # 500 -> 510\n",
    "    start_epoch=start_epoch,\n",
    "    max_memory=2000,\n",
    "    batch_size=16,\n",
    "    target_update_steps=500,\n",
    "    checkpoint_every=50,              # save often\n",
    "    max_steps_per_episode=120,        # a bit smaller -> faster\n",
    "    log_every=1                       # print each epoch so we know it's moving\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint Progress\n",
    "Check the latest saved checkpoint after training to confirm progress and log the current epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt, start_epoch = latest_checkpoint()\n",
    "print(\"Now at checkpoint:\", ckpt, \"| start_epoch:\", start_epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resuming Training with Helper Function\n",
    "Use the custom `resume_model()` helper to automatically reload the last checkpoint  \n",
    "and continue training to the next target epoch (e.g., 600).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, start_epoch = resume_model()   # helper I gave earlier\n",
    "model = qtrain(\n",
    "    model, qmaze,\n",
    "    n_epoch_total=600,    # next absolute target\n",
    "    start_epoch=start_epoch,\n",
    "    max_memory=2000,\n",
    "    batch_size=16,\n",
    "    target_update_steps=500,\n",
    "    checkpoint_every=100,\n",
    "    max_steps_per_episode=120,\n",
    "    log_every=20\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training to 5000 Total Epochs\n",
    "After validating the first run (2000), scale training to 5000 epochs total.  \n",
    "This staged approach avoids losing progress if the environment disconnects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next chunk: 5000 total\n",
    "model, _ = train_to(5000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Training Target (15,000 Epochs)\n",
    "Train the model to the full target of 15,000 epochs,  \n",
    "as this typically allows the agent to converge to a stable and high win rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final target: 15000 total\n",
    "model, _ = train_to(15000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation and Demonstration\n",
    "After training completes:\n",
    "- Evaluate the model across 100 random start episodes to calculate average win rate.  \n",
    "- Run a demo from a random start cell and from the fixed start `(0,0)` to show the learned path.  \n",
    "- Perform a full completion check to confirm the agent can consistently reach the treasure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation and Demonstration\n",
    "After training completes:\n",
    "- Evaluate the model across 100 random start episodes to calculate average win rate.  \n",
    "- Run a demo from a random start cell and from the fixed start `(0,0)` to show the learned path.  \n",
    "- Perform a full completion check to confirm the agent can consistently reach the treasure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wr = evaluate(model, qmaze, n_episodes=100)\n",
    "print(\"Final eval win-rate:\", wr)\n",
    "\n",
    "# Random start demo\n",
    "start_demo = random.choice(qmaze.free_cells)\n",
    "won, path = play_game(model, qmaze, pirate_cell=start_demo)\n",
    "print(\"Random start:\", start_demo, \"Won?\", won, \"Path len:\", len(path))\n",
    "show(qmaze)\n",
    "\n",
    "# Fixed start demo\n",
    "won, path = play_game(model, qmaze, pirate_cell=(0,0))\n",
    "print(\"Start (0,0) Won?\", won, \"Path len:\", len(path))\n",
    "show(qmaze)\n",
    "\n",
    "# Completion check\n",
    "print(\"Completion Check:\", completion_check(model, qmaze))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
